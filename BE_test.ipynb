{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "A7mF52FUxXAW",
        "outputId": "64261078-18da-4d58-f4fd-562ec2243773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tyes\n"
          ]
        }
      ],
      "source": [
        "'''Get Vector Data Base and Dependencies'''\n",
        "!pip install -U langchain-community\n",
        "!pip install sentence-transformers\n",
        "!pip install chromadb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "q7ZS-MmIvZQA"
      },
      "outputs": [],
      "source": [
        "'''Set Up the document Ingestion and Vectorization process'''\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def load_text(file_path: str) -> str:\n",
        "    \"\"\"Load a .txt\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        return f.read()\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Clean text by removing bullets and whitespace.\"\"\"\n",
        "    text = text.replace('â€¢', '')\n",
        "    text = re.sub(r'\\r\\n', '\\n', text)\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "    return text.strip()\n",
        "\n",
        "def split_by_heading(text: str):\n",
        "    \"\"\"\n",
        "    Split text into sections based on headings like '1.', '2.1', etc.\n",
        "    Returns a list of (heading, section_text).\n",
        "    \"\"\"\n",
        "    lines = text.split('\\n')\n",
        "    sections = []\n",
        "    current_heading = 'Introduction'\n",
        "    current_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        if re.match(r'^\\d+(\\.\\d+)*\\s', line):\n",
        "            # Save previous section\n",
        "            if current_lines:\n",
        "                sections.append((current_heading, '\\n'.join(current_lines).strip()))\n",
        "            current_heading = line.strip()\n",
        "            current_lines = []\n",
        "        else:\n",
        "            current_lines.append(line)\n",
        "    if current_lines:\n",
        "        sections.append((current_heading, '\\n'.join(current_lines).strip()))\n",
        "\n",
        "    return sections\n",
        "\n",
        "def chunk_section(heading: str, content: str, chunk_size: int = 500, overlap: int = 100):\n",
        "    \"\"\"\n",
        "    Chunk a section into overlapping word-based chunks.\n",
        "    Returns a list of dicts with metadata and chunk text.\n",
        "    \"\"\"\n",
        "    words = content.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    chunk_id = 0\n",
        "\n",
        "    while start < len(words):\n",
        "        end = min(start + chunk_size, len(words))\n",
        "        chunk_words = words[start:end]\n",
        "        chunk_text = ' '.join(chunk_words)\n",
        "        chunks.append({\n",
        "            'heading': heading,\n",
        "            'chunk_id': chunk_id,\n",
        "            'word_count': len(chunk_words),\n",
        "            'text': chunk_text\n",
        "        })\n",
        "        chunk_id += 1\n",
        "        start += (chunk_size - overlap)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def preprocess_file(file_path: str, chunk_size: int = 500, overlap: int = 100) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Full preprocessing pipeline: load, clean, split, and chunk.\n",
        "    Returns a pandas DataFrame with all chunks and metadata.\n",
        "    \"\"\"\n",
        "    raw_text = load_text(file_path)\n",
        "    cleaned = clean_text(raw_text)\n",
        "    sections = split_by_heading(cleaned)\n",
        "\n",
        "    all_chunks = []\n",
        "    for heading, content in sections:\n",
        "        all_chunks.extend(chunk_section(heading, content, chunk_size, overlap))\n",
        "\n",
        "    return pd.DataFrame(all_chunks)\n",
        "\n",
        "# Example usage:\n",
        "# df_chunks = preprocess_file('path/to/your/document.txt', chunk_size=200, overlap=50)\n",
        "# df_chunks.to_csv('preprocessed_chunks.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rlUT7MyveaL"
      },
      "outputs": [],
      "source": [
        "df_chunks = preprocess_file('/content/drive/MyDrive/TESTTST/sample.txt', chunk_size=200, overlap=50)\n",
        "df_chunks.to_csv('preprocessed_chunks.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl8bz5vS5R9d",
        "outputId": "b6c6d4d7-8b37-4e0b-9b6b-7810a05d9982"
      },
      "outputs": [],
      "source": [
        "'''Set up Vector DB'''\n",
        "import chromadb\n",
        "client = chromadb.Client()\n",
        "collection = client.get_or_create_collection(\"reguai_compliance\")\n",
        "client.delete_collection(\"reguai_compliance\")\n",
        "names = client.list_collections()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "\n",
        "# 1. Assume df_chunks is your preprocessed DataFrame\n",
        "#    with columns: 'text', 'heading', 'chunk_id'\n",
        "\n",
        "# 2. Initialize embedder\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# 3. Convert to lists\n",
        "texts     = df_chunks['text'].tolist()\n",
        "metadatas = df_chunks[['heading', 'chunk_id']].to_dict('records')\n",
        "ids       = [f\"chunk_{i}\" for i in range(len(texts))]\n",
        "\n",
        "# 4. Instantiate the new PersistentClient\n",
        "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# 5. Create or retrieve collection\n",
        "collection = client.get_or_create_collection(\"reguai_compliance\")\n",
        "\n",
        "# 6. Embed & add to ChromaDB\n",
        "embeddings = embedder.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "collection.add(ids=ids, documents=texts, embeddings=embeddings, metadatas=metadatas)\n",
        "\n",
        "# 7. Query example\n",
        "query = \"What is data minimisation?\"\n",
        "q_emb  = embedder.encode([query])\n",
        "res    = collection.query(query_embeddings=q_emb, n_results=3)\n",
        "print(res['documents'], res['metadatas'])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
